import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns
import os
import pickle
from datetime import datetime
import io
from sklearn.preprocessing import MinMaxScaler
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'sans-serif']
matplotlib.rcParams['axes.unicode_minus'] = False
import importlib.util
import tempfile
import yaml
import os

def run():
    # 1. æ•°æ®ä¸Šä¼ 
    st.header("ğŸ§  æ·±åº¦å­¦ä¹ æ—¶åºé¢„æµ‹")
    file = st.file_uploader("ä¸Šä¼ æ—¶åºæ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒ CSV, Excelï¼‰", type=["csv", "xls", "xlsx"], key="dl_ts_upload")
    if file is not None:
        if file.name.endswith('.csv'):
            df = pd.read_csv(file)
        elif file.name.endswith('.xls') or file.name.endswith('.xlsx'):
            df = pd.read_excel(file)
        else:
            st.error("ä»…æ”¯æŒCSVæˆ–Excelæ–‡ä»¶ï¼")
            st.stop()
        st.success(f"å·²è¯»å–æ–‡ä»¶ï¼š{file.name}")
        st.dataframe(df.head())
    else:
        st.info("è¯·ä¸Šä¼ æ•°æ®æ–‡ä»¶")
        st.stop()

    # 2. æ—¶é—´åˆ—é€‰æ‹©
    with st.expander("1ï¸âƒ£ æ—¶é—´åˆ—é€‰æ‹©"):
        time_cols = [col for col in df.columns if pd.api.types.is_datetime64_any_dtype(df[col]) or 'date' in col.lower() or 'time' in col.lower()]
        time_col = st.selectbox("é€‰æ‹©æ—¶é—´åˆ—ï¼ˆå¿…é¡»å”¯ä¸€ä¸”é€’å¢ï¼‰", time_cols if time_cols else df.columns, key="dl_ts_time_col")
        df[time_col] = pd.to_datetime(df[time_col], errors='coerce')
        df = df.sort_values(by=time_col).reset_index(drop=True)
        st.write(f"æ•°æ®æŒ‰ {time_col} å‡åºæ’åº")

    # 3. ç‰¹å¾/ç›®æ ‡é€‰æ‹©
    with st.expander("2ï¸âƒ£ ç‰¹å¾ä¸ç›®æ ‡é€‰æ‹©"):
        all_cols = [c for c in df.columns if c != time_col]
        label_col = st.selectbox("é€‰æ‹©ç›®æ ‡ï¼ˆæ ‡ç­¾ï¼‰åˆ—", all_cols, key="dl_ts_label")
        feature_cols = st.multiselect("é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆå¯å¤šé€‰ï¼‰", [c for c in all_cols if c != label_col], default=[c for c in all_cols if c != label_col], key="dl_ts_feats")
        if not feature_cols or not label_col:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç‰¹å¾å’Œä¸€ä¸ªæ ‡ç­¾ï¼")
            st.stop()

    # 3.1 å½’ä¸€åŒ–
    st.markdown("**ç‰¹å¾ä¸ç›®æ ‡å½’ä¸€åŒ–ï¼ˆMinMaxScaler 0-1ï¼‰**")
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    df[feature_cols] = scaler_X.fit_transform(df[feature_cols])
    df[label_col] = scaler_y.fit_transform(df[[label_col]])
    st.write("å½’ä¸€åŒ–åæ ·ä¾‹ï¼š")
    st.dataframe(df[feature_cols + [label_col]].head())

    # 4. é¢„æµ‹ç±»å‹ä¸çª—å£è®¾ç½®
    with st.expander("3ï¸âƒ£ é¢„æµ‹ç±»å‹ä¸çª—å£è®¾ç½®"):
        pred_type = st.selectbox("é€‰æ‹©æ—¶åºé¢„æµ‹ç±»å‹", [
            "å•å˜é‡å•æ­¥é¢„æµ‹ï¼ˆç›®æ ‡å˜é‡æ»åç‰¹å¾ï¼‰",
            "å¤šå˜é‡å•æ­¥é¢„æµ‹ï¼ˆç›®æ ‡å˜é‡æ»åç‰¹å¾ï¼‰",
            "å¤šå˜é‡å¤šæ­¥é¢„æµ‹ï¼ˆç›®æ ‡å˜é‡æ»åç‰¹å¾+å¤šæ­¥æ ‡ç­¾ï¼‰"
        ])
        lag_num = st.slider("æ»åæ­¥æ•°ï¼ˆè¾“å…¥çª—å£é•¿åº¦ï¼‰", 1, 48, 12)
        multi_step = 1
        if pred_type == "å¤šå˜é‡å¤šæ­¥é¢„æµ‹ï¼ˆç›®æ ‡å˜é‡æ»åç‰¹å¾+å¤šæ­¥æ ‡ç­¾ï¼‰":
            multi_step = st.slider("å¤šæ­¥é¢„æµ‹æ­¥æ•°ï¼ˆè¾“å‡ºæ­¥æ•°ï¼‰", 2, 24, 3)
        use_lag_target = st.checkbox("å°†ç›®æ ‡å˜é‡çš„å‰Næ­¥ä½œä¸ºé¢å¤–ç‰¹å¾", value=True)

    # 5. æ„å»ºæ·±åº¦å­¦ä¹ è¾“å…¥æ•°æ®
    st.subheader("æ•°æ®é¢„å¤„ç†ä¸çª—å£æ„å»º")
    def build_dl_windows(df, feature_cols, label_col, lag_num, multi_step=1, pred_type="å•å˜é‡å•æ­¥é¢„æµ‹ï¼ˆç›®æ ‡å˜é‡æ»åç‰¹å¾ï¼‰", use_lag_target=True):
        X, y = [], []
        values = df[feature_cols + [label_col]].values
        label_values = df[label_col].values
        for i in range(lag_num, len(df) - multi_step + 1):
            if pred_type == "å•å˜é‡å•æ­¥é¢„æµ‹ï¼ˆç›®æ ‡å˜é‡æ»åç‰¹å¾ï¼‰":
                X.append(label_values[i-lag_num:i].reshape(-1, 1))
                y.append(label_values[i + multi_step - 1])
            else:
                # å‰Næ­¥æ‰€æœ‰ç‰¹å¾ (N, ç‰¹å¾æ•°)
                X_window = values[i-lag_num:i, :-1]
                if use_lag_target:
                    lagged_targets = label_values[i-lag_num:i].reshape(-1, 1)  # (N, 1)
                    X_full = np.concatenate([X_window, lagged_targets], axis=1)  # (N, ç‰¹å¾æ•°+1)
                else:
                    X_full = X_window
                X.append(X_full)
                if multi_step == 1:
                    y.append(values[i, -1])
                else:
                    y.append([values[i + j, -1] for j in range(multi_step)])
        X = np.array(X)
        y = np.array(y)
        return X, y

    X, y = build_dl_windows(df, feature_cols, label_col, lag_num, multi_step, pred_type, use_lag_target)
    st.write(f"è¾“å…¥X shape: {X.shape}, æ ‡ç­¾y shape: {y.shape}")

    # é¢„è§ˆçª—å£ç‰¹å¾ä¸æ ‡ç­¾
    st.markdown("**çª—å£ç‰¹å¾ä¸æ ‡ç­¾é¢„è§ˆï¼š**")
    col1, col2 = st.columns(2)
    with col1:
        st.write("ç‰¹å¾çª—å£ (X[0]):")
        if pred_type == "å•å˜é‡å•æ­¥é¢„æµ‹ï¼ˆç›®æ ‡å˜é‡æ»åç‰¹å¾ï¼‰":
            preview_cols = [label_col]
            st.dataframe(pd.DataFrame(X[0], columns=preview_cols))
        else:
            base_cols = feature_cols.copy()
            if use_lag_target:
                preview_cols = base_cols + [label_col]
            else:
                preview_cols = base_cols
            st.dataframe(pd.DataFrame(X[0], columns=preview_cols))
    with col2:
        st.write("æ ‡ç­¾ (y[0]):")
        if y.ndim == 1:
            st.write(y[0])
        else:
            st.write(y[0])

    # 6. è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†
    with st.expander("4ï¸âƒ£ è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰"):
        test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.05, 0.5, 0.2, 0.05)
        n = len(X)
        n_test = int(n * test_size)
        n_train = n - n_test
        st.write(f"è®­ç»ƒé›†ï¼š{n_train}ï¼Œæµ‹è¯•é›†ï¼š{n_test}")
        X_train, X_test = X[:n_train], X[n_train:]
        y_train, y_test = y[:n_train], y[n_train:]
        time_test = df[time_col].iloc[-n_test: ] if n_test > 0 else df[time_col].iloc[-len(X_test):]

    # å…è®¸ç”¨æˆ·ä¸Šä¼ è‡ªå®šä¹‰æ¨¡å‹.pyæ–‡ä»¶
    st.markdown("**å¯é€‰ï¼šä¸Šä¼ è‡ªå®šä¹‰PyTorchæ¨¡å‹ï¼ˆ.pyæ–‡ä»¶ï¼Œéœ€ç»§æ‰¿nn.Moduleï¼Œæ„é€ å‚æ•°ä¸º(input_size, hidden_size, num_layers, output_size, dropout, **kwargs)ï¼‰**")
    uploaded_model_file = st.file_uploader("ä¸Šä¼ è‡ªå®šä¹‰æ¨¡å‹.pyæ–‡ä»¶", type=["py"], key="custom_model_upload")
    uploaded_models = {}
    if uploaded_model_file is not None:
        custom_model_dir = os.path.join(os.path.dirname(__file__), "custom_models")
        os.makedirs(custom_model_dir, exist_ok=True)
        save_path = os.path.join(custom_model_dir, uploaded_model_file.name)
        with open(save_path, "wb") as f:
            f.write(uploaded_model_file.read())
        module_name = os.path.splitext(uploaded_model_file.name)[0]
        spec = importlib.util.spec_from_file_location(module_name, save_path)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        for attr in dir(module):
            cls = getattr(module, attr)
            if isinstance(cls, type) and issubclass(cls, nn.Module) and cls is not nn.Module:
                uploaded_models[attr] = cls
        st.success(f"å·²åŠ è½½è‡ªå®šä¹‰æ¨¡å‹: {', '.join(uploaded_models.keys())}")

    # å…è®¸ç”¨æˆ·ä¸Šä¼ è‡ªå®šä¹‰æ¨¡å‹å‚æ•°yamlæ–‡ä»¶
    st.markdown("**å¯é€‰ï¼šä¸Šä¼ è‡ªå®šä¹‰æ¨¡å‹å‚æ•°ï¼ˆ.yamlæ–‡ä»¶ï¼Œå†…å®¹ä¸ºæ¨¡å‹æ„é€ å‚æ•°å­—å…¸ï¼‰**")
    uploaded_yaml_file = st.file_uploader("ä¸Šä¼ è‡ªå®šä¹‰æ¨¡å‹å‚æ•°.yamlæ–‡ä»¶", type=["yaml", "yml"], key="custom_model_yaml_upload")
    yaml_params = {}
    if uploaded_yaml_file is not None:
        yaml_params = yaml.safe_load(uploaded_yaml_file)
        st.info(f"å·²åŠ è½½è‡ªå®šä¹‰å‚æ•°: {yaml_params}")

    # è‡ªåŠ¨åŠ è½½custom_modelsç›®å½•ä¸‹æ¨¡å‹
    custom_model_dir = os.path.join(os.path.dirname(__file__), "custom_models")
    custom_models = {}
    if os.path.exists(custom_model_dir):
        for fname in os.listdir(custom_model_dir):
            if fname.endswith(".py"):
                module_name = fname[:-3]
                spec = importlib.util.spec_from_file_location(module_name, os.path.join(custom_model_dir, fname))
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                for attr in dir(module):
                    cls = getattr(module, attr)
                    if isinstance(cls, type) and issubclass(cls, nn.Module) and cls is not nn.Module:
                        custom_models[attr] = cls

    # 7. æ¨¡å‹é€‰æ‹©ä¸å‚æ•°è®¾ç½®
    with st.expander("5ï¸âƒ£ æ¨¡å‹é€‰æ‹©ä¸å‚æ•°è®¾ç½®"):
        builtin_models = [
            "LSTM",
            "GRU",
            "MLP",
            "1D-CNN",
            "Transformer"
        ]
        model_choices = builtin_models + list(custom_models.keys()) + list(uploaded_models.keys())
        model_type = st.selectbox("é€‰æ‹©æ·±åº¦å­¦ä¹ æ¨¡å‹ç»“æ„", model_choices)
        hidden_size = st.slider("éšè—å•å…ƒæ•°/é€šé“æ•°", 8, 256, 64)
        num_layers = st.slider("å±‚æ•°", 1, 4, 1)
        dropout = st.slider("Dropout", 0.0, 0.5, 0.1)
        epochs = st.slider("è®­ç»ƒè½®æ•°", 5, 200, 30)
        batch_size = st.slider("Batch size", 8, 256, 32)
        lr = st.number_input("å­¦ä¹ ç‡", 1e-5, 1e-1, 1e-3, format="%e")
        # æ–°å¢æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨é€‰æ‹©
        loss_options = {"MSELoss": nn.MSELoss, "L1Loss (MAE)": nn.L1Loss, "HuberLoss": nn.SmoothL1Loss, "CrossEntropyLoss (ä»…åˆ†ç±»)": nn.CrossEntropyLoss}
        optimizer_options = {"Adam": optim.Adam, "SGD": optim.SGD, "RMSprop": optim.RMSprop, "Adagrad": optim.Adagrad}
        loss_choice = st.selectbox("é€‰æ‹©æŸå¤±å‡½æ•°", list(loss_options.keys()), index=0, help="CrossEntropyLoss ä»…é€‚ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œå›å½’è¯·å‹¿é€‰")
        optimizer_choice = st.selectbox("é€‰æ‹©ä¼˜åŒ–å™¨", list(optimizer_options.keys()), index=0)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        st.write(f"å½“å‰è®¾å¤‡: {device}")

    # 8. å®šä¹‰æ¨¡å‹ç»“æ„ï¼ˆå¯æ‰©å±•ï¼‰
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
            super().__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, output_size)
        def forward(self, x):
            out, _ = self.lstm(x)
            out = out[:, -1, :]
            out = self.fc(out)
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
            super().__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, output_size)
        def forward(self, x):
            out, _ = self.gru(x)
            out = out[:, -1, :]
            out = self.fc(out)
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
            super().__init__()
            layers = [nn.Flatten()]
            in_dim = input_size
            for _ in range(num_layers):
                layers.append(nn.Linear(in_dim, hidden_size))
                layers.append(nn.ReLU())
                layers.append(nn.Dropout(dropout))
                in_dim = hidden_size
            layers.append(nn.Linear(hidden_size, output_size))
            self.net = nn.Sequential(*layers)
        def forward(self, x):
            return self.net(x)

    class CNN1DModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
            super().__init__()
            layers = []
            in_channels = input_size
            for _ in range(num_layers):
                layers.append(nn.Conv1d(in_channels, hidden_size, kernel_size=3, padding=1))
                layers.append(nn.ReLU())
                layers.append(nn.Dropout(dropout))
                in_channels = hidden_size
            self.conv = nn.Sequential(*layers)
            self.fc = nn.Linear(hidden_size * lag_num, output_size)
        def forward(self, x):
            # x: (batch, seq, feat) -> (batch, feat, seq)
            x = x.permute(0, 2, 1)
            out = self.conv(x)
            out = out.reshape(out.size(0), -1)
            out = self.fc(out)
            return out

    class TransformerModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
            super().__init__()
            encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=4, dim_feedforward=hidden_size, dropout=dropout, batch_first=True)
            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
            self.fc = nn.Linear(input_size, output_size)
        def forward(self, x):
            # x: (batch, seq, feat)
            out = self.transformer(x)
            out = out[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            out = self.fc(out)
            return out

    # 9. è®­ç»ƒä¸è¯„ä¼°
    if st.button("å¼€å§‹è®­ç»ƒ"):
        st.info("æ•°æ®å‡†å¤‡ä¸­...")
        X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)
        y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)
        X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)
        y_test_t = torch.tensor(y_test, dtype=torch.float32).to(device)
        train_ds = TensorDataset(X_train_t, y_train_t)
        test_ds = TensorDataset(X_test_t, y_test_t)
        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_ds, batch_size=batch_size)

        input_size = X_train.shape[2] if len(X_train.shape) == 3 else X_train.shape[1]
        output_size = y_train.shape[1] if len(y_train.shape) > 1 else 1
        if model_type == "LSTM":
            model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout).to(device)
        elif model_type == "GRU":
            model = GRUModel(input_size, hidden_size, num_layers, output_size, dropout).to(device)
        elif model_type == "MLP":
            model = MLPModel(input_size * lag_num, hidden_size, num_layers, output_size, dropout).to(device)
        elif model_type == "1D-CNN":
            model = CNN1DModel(input_size, hidden_size, num_layers, output_size, dropout).to(device)
        elif model_type == "Transformer":
            model = TransformerModel(input_size, hidden_size, num_layers, output_size, dropout).to(device)
        elif model_type in custom_models:
            model = custom_models[model_type](input_size, hidden_size, num_layers, output_size, dropout, **yaml_params).to(device)
        elif model_type in uploaded_models:
            model = uploaded_models[model_type](input_size, hidden_size, num_layers, output_size, dropout, **yaml_params).to(device)
        else:
            st.error("æœªçŸ¥æ¨¡å‹ç±»å‹")
            st.stop()

        # æ ¹æ®é€‰æ‹©æ„å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = loss_options[loss_choice]()
        optimizer = optimizer_options[optimizer_choice](model.parameters(), lr=lr)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

        st.info("å¼€å§‹è®­ç»ƒ...")
        progress_bar = st.progress(0)
        losses = []
        lrs = []
        train_scores = []
        test_scores = []
        # åŠ¨æ€å¯è§†åŒ–å®¹å™¨
        chart_row = st.columns(4)
        acc_chart = chart_row[0].empty()
        test_acc_chart = chart_row[1].empty()
        loss_chart = chart_row[2].empty()
        lr_chart = chart_row[3].empty()

        for epoch in range(epochs):
            model.train()
            epoch_loss = 0
            for xb, yb in train_loader:
                optimizer.zero_grad()
                if model_type == "MLP":
                    xb = xb.reshape(xb.size(0), -1)
                out = model(xb)
                loss = criterion(out, yb if output_size > 1 else yb.view(-1, 1))
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item() * xb.size(0)
            epoch_loss /= len(train_loader.dataset)
            losses.append(epoch_loss)
            lrs.append(optimizer.param_groups[0]['lr'])
            # è®­ç»ƒå‡†ç¡®åº¦ï¼ˆRÂ²ï¼‰
            model.eval()
            with torch.no_grad():
                if model_type == "MLP":
                    X_train_eval = X_train_t.reshape(X_train_t.size(0), -1)
                    X_test_eval = X_test_t.reshape(X_test_t.size(0), -1)
                else:
                    X_train_eval = X_train_t
                    X_test_eval = X_test_t
                y_train_pred = model(X_train_eval).cpu().numpy()
                y_test_pred = model(X_test_eval).cpu().numpy()
                # åå½’ä¸€åŒ–
                if y_train_pred.ndim == 1 or y_train_pred.shape[1] == 1:
                    y_train_pred_inv = scaler_y.inverse_transform(y_train_pred.reshape(-1, 1)).squeeze()
                    y_train_true_inv = scaler_y.inverse_transform(y_train.reshape(-1, 1)).squeeze()
                    y_test_pred_inv = scaler_y.inverse_transform(y_test_pred.reshape(-1, 1)).squeeze()
                    y_test_true_inv = scaler_y.inverse_transform(y_test.reshape(-1, 1)).squeeze()
                else:
                    y_train_pred_inv = scaler_y.inverse_transform(y_train_pred)
                    y_train_true_inv = scaler_y.inverse_transform(y_train)
                    y_test_pred_inv = scaler_y.inverse_transform(y_test_pred)
                    y_test_true_inv = scaler_y.inverse_transform(y_test)
                if output_size == 1:
                    train_r2 = 1 - np.sum((y_train_true_inv - y_train_pred_inv) ** 2) / np.sum((y_train_true_inv - np.mean(y_train_true_inv)) ** 2)
                    test_r2 = 1 - np.sum((y_test_true_inv - y_test_pred_inv) ** 2) / np.sum((y_test_true_inv - np.mean(y_test_true_inv)) ** 2)
                else:
                    train_r2 = np.mean([1 - np.sum((y_train_true_inv[:, i] - y_train_pred_inv[:, i]) ** 2) / np.sum((y_train_true_inv[:, i] - np.mean(y_train_true_inv[:, i])) ** 2) for i in range(output_size)])
                    test_r2 = np.mean([1 - np.sum((y_test_true_inv[:, i] - y_test_pred_inv[:, i]) ** 2) / np.sum((y_test_true_inv[:, i] - np.mean(y_test_true_inv[:, i])) ** 2) for i in range(output_size)])
                train_scores.append(train_r2)
                test_scores.append(test_r2)
            scheduler.step(epoch_loss)
            # åŠ¨æ€åˆ·æ–°æ›²çº¿
            with acc_chart:
                fig_train_acc, ax_train_acc = plt.subplots(figsize=(3, 2))
                ax_train_acc.plot(train_scores, label='è®­ç»ƒRÂ²')
                ax_train_acc.set_title('è®­ç»ƒé›†R$^2$')
                ax_train_acc.set_xlim(0, epochs)
                ax_train_acc.set_ylim(0, 1)
                ax_train_acc.set_xlabel('Epoch')
                ax_train_acc.set_ylabel('R$^2$')
                acc_chart.pyplot(fig_train_acc)
            with test_acc_chart:
                fig_test_acc, ax_test_acc = plt.subplots(figsize=(3, 2))
                ax_test_acc.plot(test_scores, label='æµ‹è¯•RÂ²', color='orange')
                ax_test_acc.set_title('æµ‹è¯•é›†R$^2$')
                ax_test_acc.set_xlim(0, epochs)
                ax_test_acc.set_ylim(0, 1)
                ax_test_acc.set_xlabel('Epoch')
                ax_test_acc.set_ylabel('R$^2$')
                test_acc_chart.pyplot(fig_test_acc)
            with loss_chart:
                fig_loss, ax_loss = plt.subplots(figsize=(3, 2))
                ax_loss.plot(losses)
                ax_loss.set_title('è®­ç»ƒæŸå¤±')
                ax_loss.set_xlabel('Epoch')
                ax_loss.set_ylabel('Loss')
                ax_loss.set_xlim(0, epochs)
                loss_chart.pyplot(fig_loss)
            with lr_chart:
                fig_lr, ax_lr = plt.subplots(figsize=(3, 2))
                ax_lr.plot(lrs)
                ax_lr.set_title('å­¦ä¹ ç‡')
                ax_lr.set_xlabel('Epoch')
                ax_lr.set_ylabel('Learning Rate')
                ax_lr.set_xlim(0, epochs)
                lr_chart.pyplot(fig_lr)
            progress_bar.progress((epoch+1)/epochs, text=f"è®­ç»ƒè¿›åº¦: {epoch+1}/{epochs}")

        progress_bar.empty()
        st.success("è®­ç»ƒå®Œæˆï¼")

        # è®­ç»ƒ/æµ‹è¯•å‡†ç¡®åº¦æ›²çº¿
        st.subheader("è®­ç»ƒ/æµ‹è¯•é›†RÂ²æ›²çº¿")
        row_acc = st.columns(2)
        with row_acc[0]:
            fig_train_acc, ax_train_acc = plt.subplots()
            ax_train_acc.plot(train_scores, label='è®­ç»ƒRÂ²')
            ax_train_acc.set_xlabel('Epoch')
            ax_train_acc.set_ylabel('RÂ²')
            ax_train_acc.set_title('è®­ç»ƒé›†RÂ²')
            st.pyplot(fig_train_acc)
        with row_acc[1]:
            fig_test_acc, ax_test_acc = plt.subplots()
            ax_test_acc.plot(test_scores, label='æµ‹è¯•RÂ²', color='orange')
            ax_test_acc.set_xlabel('Epoch')
            ax_test_acc.set_ylabel('RÂ²')
            ax_test_acc.set_title('æµ‹è¯•é›†RÂ²')
            st.pyplot(fig_test_acc)

        # æŸå¤±å’Œå­¦ä¹ ç‡æ›²çº¿
        row_loss = st.columns(2)
        with row_loss[0]:
            st.subheader("è®­ç»ƒæŸå¤±æ›²çº¿")
            fig_loss, ax_loss = plt.subplots()
            ax_loss.plot(losses)
            ax_loss.set_xlabel('Epoch')
            ax_loss.set_ylabel('Loss')
            st.pyplot(fig_loss)
        with row_loss[1]:
            st.subheader("å­¦ä¹ ç‡å˜åŒ–æ›²çº¿")
            fig_lr, ax_lr = plt.subplots()
            ax_lr.plot(lrs)
            ax_lr.set_xlabel('Epoch')
            ax_lr.set_ylabel('Learning Rate')
            st.pyplot(fig_lr)

        # è¯„ä¼°
        model.eval()
        with torch.no_grad():
            if model_type == "MLP":
                X_test_eval = X_test_t.reshape(X_test_t.size(0), -1)
            else:
                X_test_eval = X_test_t
            y_pred = model(X_test_eval).cpu().numpy()
            y_true = y_test
        # åå½’ä¸€åŒ–
        if y_pred.ndim == 1 or y_pred.shape[1] == 1:
            y_pred_inv = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).squeeze()
            y_true_inv = scaler_y.inverse_transform(y_true.reshape(-1, 1)).squeeze()
        else:
            y_pred_inv = scaler_y.inverse_transform(y_pred)
            y_true_inv = scaler_y.inverse_transform(y_true)
        # ä¸»è¦æŒ‡æ ‡
        if output_size == 1:
            mse = np.mean((y_true_inv - y_pred_inv) ** 2)
            mae = np.mean(np.abs(y_true_inv - y_pred_inv))
            r2 = 1 - np.sum((y_true_inv - y_pred_inv) ** 2) / np.sum((y_true_inv - np.mean(y_true_inv)) ** 2)
            st.write(f"MSE: {mse:.4f}  |  MAE: {mae:.4f}  |  RÂ²: {r2:.4f}")
        else:
            metrics = []
            for i in range(output_size):
                mse = np.mean((y_true_inv[:, i] - y_pred_inv[:, i]) ** 2)
                mae = np.mean(np.abs(y_true_inv[:, i] - y_pred_inv[:, i]))
                r2 = 1 - np.sum((y_true_inv[:, i] - y_pred_inv[:, i]) ** 2) / np.sum((y_true_inv[:, i] - np.mean(y_true_inv[:, i])) ** 2)
                metrics.append([mse, mae, r2])
            metrics_df = pd.DataFrame(metrics, columns=["MSE", "MAE", "RÂ²"], index=[f"step_{i+1}" for i in range(output_size)])
            st.dataframe(metrics_df.style.format("{:.4f}"), use_container_width=True)
        # å¯è§†åŒ–
        st.subheader("æµ‹è¯•é›†é¢„æµ‹-çœŸå®å€¼å¯¹æ¯”")
        if output_size == 1:
            fig_pred, ax_pred = plt.subplots()
            ax_pred.plot(time_test, y_true_inv, label='çœŸå®å€¼', marker='o', color='blue')
            ax_pred.plot(time_test, y_pred_inv, label='é¢„æµ‹å€¼', marker='x', color='orange')
            ax_pred.set_xlabel('æ—¶é—´')
            ax_pred.set_ylabel('æ•°å€¼')
            ax_pred.legend()
            st.pyplot(fig_pred)
        else:
            for i in range(output_size):
                fig_pred, ax_pred = plt.subplots()
                ax_pred.plot(time_test, y_true_inv[:, i], label='çœŸå®å€¼', marker='o', color='blue')
                ax_pred.plot(time_test, y_pred_inv[:, i], label='é¢„æµ‹å€¼', marker='x', color='orange')
                ax_pred.set_xlabel('æ—¶é—´')
                ax_pred.set_ylabel('æ•°å€¼')
                ax_pred.set_title(f'æ­¥é•¿ step_{i+1}')
                ax_pred.legend()
                st.pyplot(fig_pred)
        # æ¨¡å‹å¯¼å‡º
        st.subheader("æ¨¡å‹å¯¼å‡º")
        # .ptå¯¼å‡º
        pt_filename = f"dl_ts_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt"
        torch.save(model.state_dict(), pt_filename)
        with open(pt_filename, "rb") as f:
            st.download_button("ä¸‹è½½ PyTorch .pt æ¨¡å‹", data=f, file_name=pt_filename)
        # .onnxå¯¼å‡ºï¼ˆå¯é€‰ï¼‰
        if st.checkbox("å¯¼å‡ºä¸ºONNXæ ¼å¼"):
            onnx_filename = pt_filename.replace('.pt', '.onnx')
            dummy_input = torch.randn(1, *X_test.shape[1:], dtype=torch.float32).to(device)
            torch.onnx.export(model, dummy_input, onnx_filename, input_names=['input'], output_names=['output'],
                              dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}},
                              opset_version=12)
            with open(onnx_filename, "rb") as f:
                st.download_button("ä¸‹è½½ ONNX æ¨¡å‹", data=f, file_name=onnx_filename)

    # 10. é¢„ç•™ï¼šåŠ¨æ€å¯¼å…¥æ–°æ¨¡å‹ç»“æ„
    # ä½ å¯ä»¥åœ¨æœ¬ç›®å½•ä¸‹æ–°å»ºè‡ªå®šä¹‰æ¨¡å‹ç±»ï¼Œå¹¶åœ¨æ¨¡å‹é€‰æ‹©åŒºè‡ªåŠ¨importå¹¶åŠ å…¥model_typeåˆ—è¡¨ã€‚
    # æœªæ¥å¯ç”¨importlibåŠ¨æ€åŠ è½½è‡ªå®šä¹‰æ¨¡å‹ã€‚
