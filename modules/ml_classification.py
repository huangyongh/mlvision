import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus'] = False

def run():
    st.header("ğŸ”¬ æœºå™¨å­¦ä¹ åˆ†ç±»å»ºæ¨¡ï¼ˆæ•°å€¼å‹æ•°æ®ï¼‰")
    # åªå…è®¸ä¸Šä¼ æ–‡ä»¶ä½œä¸ºæ•°æ®æº
    file = st.file_uploader("ä¸Šä¼ æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒ CSV, Excelï¼‰", type=["csv", "xls", "xlsx"], key="clf_upload")
    if file is not None:
        if file.name.endswith('.csv'):
            df = pd.read_csv(file)
        elif file.name.endswith('.xls') or file.name.endswith('.xlsx'):
            df = pd.read_excel(file)
        else:
            st.error("ä»…æ”¯æŒCSVæˆ–Excelæ–‡ä»¶ï¼")
            st.stop()
        st.success(f"å·²è¯»å–æ–‡ä»¶ï¼š{file.name}")
        st.dataframe(df.head())
    else:
        st.info("è¯·ä¸Šä¼ æ•°æ®æ–‡ä»¶")
        st.stop()

    # 2. ç‰¹å¾/æ ‡ç­¾é€‰æ‹©
    with st.expander("1ï¸âƒ£ ç‰¹å¾ä¸æ ‡ç­¾é€‰æ‹©"):
        all_cols = df.columns.tolist()
        label_col = st.selectbox("é€‰æ‹©ç›®æ ‡ï¼ˆæ ‡ç­¾ï¼‰åˆ—", all_cols, key="clf_label")
        feature_cols = st.multiselect("é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆå¯å¤šé€‰ï¼‰", [c for c in all_cols if c != label_col], default=[c for c in all_cols if c != label_col], key="clf_feats")
        if not feature_cols or not label_col:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç‰¹å¾å’Œä¸€ä¸ªæ ‡ç­¾ï¼")
            st.stop()

    # æ ‡ç­¾åˆ†å¸ƒæ£€æŸ¥
    y = df[label_col]
    st.write('æ ‡ç­¾åˆ†å¸ƒï¼š')
    st.write(y.value_counts())
    if y.value_counts().min() < 2:
        st.error("æ ‡ç­¾ä¸­æœ‰ç±»åˆ«æ ·æœ¬æ•°å°äº2ï¼Œæ— æ³•è¿›è¡Œåˆ†å±‚é‡‡æ ·å’Œäº¤å‰éªŒè¯ï¼Œè¯·æ£€æŸ¥æ•°æ®ï¼")
        st.stop()

    # 3. è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†
    with st.expander("2ï¸âƒ£ è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†"):
        test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.2, 0.05)
        random_state = st.number_input("éšæœºç§å­", 0, 9999, 42)
        X = df[feature_cols]
        # y = df[label_col]  # å·²æå‰å®šä¹‰
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)
        st.write(f"è®­ç»ƒé›†ï¼š{X_train.shape}ï¼Œæµ‹è¯•é›†ï¼š{X_test.shape}")

    # 4. æ¨¡å‹é€‰æ‹©
    with st.expander("3ï¸âƒ£ æ¨¡å‹é€‰æ‹©"):
        model_dict = {
            "é€»è¾‘å›å½’": LogisticRegression,
            "Kè¿‘é‚»": KNeighborsClassifier,
            "å†³ç­–æ ‘": DecisionTreeClassifier,
            "éšæœºæ£®æ—": RandomForestClassifier,
            "æ¢¯åº¦æå‡æ ‘": GradientBoostingClassifier,
            "XGBoost": XGBClassifier,
            "LightGBM": LGBMClassifier,
            "æ”¯æŒå‘é‡æœº": SVC,
            "æœ´ç´ è´å¶æ–¯": GaussianNB,
        }
        model_names = list(model_dict.keys())
        selected_models = st.multiselect("é€‰æ‹©è¦å¯¹æ¯”çš„æ¨¡å‹ï¼ˆå¯å¤šé€‰ï¼‰", model_names, default=["é€»è¾‘å›å½’", "éšæœºæ£®æ—"])
        if not selected_models:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼")
            st.stop()

    # 4.1 æ¯ä¸ªæ¨¡å‹å‚æ•°è®¾ç½®
    model_params = {}
    for m in selected_models:
        with st.expander(f"{m} å‚æ•°è®¾ç½®"):
            params = {}
            if m == "é€»è¾‘å›å½’":
                C = st.number_input(f"Cï¼ˆæ­£åˆ™åŒ–å¼ºåº¦ï¼Œè¶Šå°æ­£åˆ™è¶Šå¼ºï¼‰", 0.001, 100.0, 1.0, key=f"{m}_C")
                solver = st.selectbox(f"solverï¼ˆä¼˜åŒ–å™¨ï¼‰", ["lbfgs", "liblinear", "saga"], key=f"{m}_solver")
                max_iter = st.number_input(f"max_iterï¼ˆæœ€å¤§è¿­ä»£æ¬¡æ•°ï¼‰", 50, 5000, 1000, key=f"{m}_max_iter")
                params = {"C": C, "solver": solver, "max_iter": max_iter}
            elif m == "Kè¿‘é‚»":
                n_neighbors = st.number_input(f"n_neighborsï¼ˆé‚»å±…æ•°ï¼‰", 1, 50, 5, key=f"{m}_n_neighbors")
                weights = st.selectbox(f"weights", ["uniform", "distance"], key=f"{m}_weights")
                params = {"n_neighbors": n_neighbors, "weights": weights}
            elif m == "å†³ç­–æ ‘":
                max_depth = st.number_input(f"max_depthï¼ˆæœ€å¤§æ·±åº¦ï¼‰", 1, 50, 5, key=f"{m}_max_depth")
                criterion = st.selectbox(f"criterion", ["gini", "entropy", "log_loss"], key=f"{m}_criterion")
                params = {"max_depth": max_depth, "criterion": criterion}
            elif m == "éšæœºæ£®æ—":
                n_estimators = st.number_input(f"n_estimatorsï¼ˆæ ‘æ•°ï¼‰", 10, 1000, 100, key=f"{m}_n_estimators")
                max_depth = st.number_input(f"max_depthï¼ˆæœ€å¤§æ·±åº¦ï¼‰", 1, 50, 5, key=f"{m}_max_depth")
                params = {"n_estimators": n_estimators, "max_depth": max_depth}
            elif m == "æ¢¯åº¦æå‡æ ‘":
                n_estimators = st.number_input(f"n_estimatorsï¼ˆæ ‘æ•°ï¼‰", 10, 1000, 100, key=f"{m}_gbdt_n_estimators")
                learning_rate = st.number_input(f"learning_rateï¼ˆå­¦ä¹ ç‡ï¼‰", 0.001, 1.0, 0.1, key=f"{m}_learning_rate")
                params = {"n_estimators": n_estimators, "learning_rate": learning_rate}
            elif m == "XGBoost":
                n_estimators = st.number_input(f"n_estimatorsï¼ˆæ ‘æ•°ï¼‰", 10, 1000, 100, key=f"{m}_xgb_n_estimators")
                learning_rate = st.number_input(f"learning_rateï¼ˆå­¦ä¹ ç‡ï¼‰", 0.001, 1.0, 0.1, key=f"{m}_xgb_learning_rate")
                use_label_encoder = False
                eval_metric = 'logloss'
                params = {"n_estimators": n_estimators, "learning_rate": learning_rate, "use_label_encoder": use_label_encoder, "eval_metric": eval_metric}
            elif m == "LightGBM":
                n_estimators = st.number_input(f"n_estimatorsï¼ˆæ ‘æ•°ï¼‰", 10, 1000, 100, key=f"{m}_lgb_n_estimators")
                learning_rate = st.number_input(f"learning_rateï¼ˆå­¦ä¹ ç‡ï¼‰", 0.001, 1.0, 0.1, key=f"{m}_lgb_learning_rate")
                params = {"n_estimators": n_estimators, "learning_rate": learning_rate}
            elif m == "æ”¯æŒå‘é‡æœº":
                C = st.number_input(f"Cï¼ˆæ­£åˆ™åŒ–å¼ºåº¦ï¼‰", 0.001, 100.0, 1.0, key=f"{m}_svm_C")
                kernel = st.selectbox(f"kernelï¼ˆæ ¸å‡½æ•°ï¼‰", ["rbf", "linear", "poly", "sigmoid"], key=f"{m}_svm_kernel")
                probability = True
                params = {"C": C, "kernel": kernel, "probability": probability}
            elif m == "æœ´ç´ è´å¶æ–¯":
                var_smoothing = st.number_input(f"var_smoothingï¼ˆæ–¹å·®å¹³æ»‘ï¼‰", 1e-12, 1e-6, 1e-9, format="%e", key=f"{m}_nb_var_smoothing")
                params = {"var_smoothing": var_smoothing}
            model_params[m] = params

    # 5. äº¤å‰éªŒè¯ä¸è‡ªåŠ¨è°ƒå‚
    with st.expander("4ï¸âƒ£ äº¤å‰éªŒè¯ä¸è‡ªåŠ¨è°ƒå‚"):
        use_cv = st.checkbox("å¯ç”¨äº¤å‰éªŒè¯", value=True)
        cv_folds = st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 3, 10, 5) if use_cv else 1
        use_search = st.checkbox("å¯ç”¨è‡ªåŠ¨è°ƒå‚ï¼ˆGridSearchCVï¼‰", value=False)
        param_grid = {}
        if use_search:
            st.info("å¦‚éœ€è‡ªåŠ¨è°ƒå‚ï¼Œè¯·ä¸ºæ¯ä¸ªæ¨¡å‹è®¾ç½®å‚æ•°èŒƒå›´ï¼ˆå¦‚ n_estimators: 50,100,200ï¼‰")
            for m in selected_models:
                if m == "éšæœºæ£®æ—":
                    param_grid[m] = {"n_estimators": st.text_input("éšæœºæ£®æ— n_estimators", "100,200,300").split(","),
                                     "max_depth": st.text_input("éšæœºæ£®æ— max_depth", "3,5,7").split(",")}
                elif m == "é€»è¾‘å›å½’":
                    param_grid[m] = {"C": st.text_input("é€»è¾‘å›å½’ C", "0.1,1,10").split(",")}
                elif m == "Kè¿‘é‚»":
                    param_grid[m] = {"n_neighbors": st.text_input("Kè¿‘é‚» n_neighbors", "3,5,7").split(",")}
                elif m == "XGBoost":
                    param_grid[m] = {"n_estimators": st.text_input("XGBoost n_estimators", "100,200").split(",")}
                elif m == "LightGBM":
                    param_grid[m] = {"n_estimators": st.text_input("LightGBM n_estimators", "100,200").split(",")}
                elif m == "æ”¯æŒå‘é‡æœº":
                    param_grid[m] = {"C": st.text_input("SVM C", "0.1,1,10").split(",")}
                elif m == "å†³ç­–æ ‘":
                    param_grid[m] = {"max_depth": st.text_input("å†³ç­–æ ‘ max_depth", "3,5,7").split(",")}
                elif m == "æ¢¯åº¦æå‡æ ‘":
                    param_grid[m] = {"n_estimators": st.text_input("GBDT n_estimators", "100,200").split(",")}
                elif m == "æœ´ç´ è´å¶æ–¯":
                    param_grid[m] = {}  # é€šå¸¸æ— å‚æ•°

    # 6. è®­ç»ƒä¸è¯„ä¼°
    if st.button("å¼€å§‹è®­ç»ƒå¹¶å¯¹æ¯”"):
        results = {}
        fig_roc, ax_roc = plt.subplots()
        progress_container = st.empty()
        info_container = st.empty()
        total = len(selected_models)
        # æ–°å¢ï¼šæ”¶é›†å„æ¨¡å‹ä¸»è¦æŒ‡æ ‡
        metrics_dict = {}
        auc_dict = {}
        test_pred_figs = []
        for idx, m in enumerate(selected_models):
            info_container.info(f"æ­£åœ¨è®­ç»ƒï¼š{m}")
            # ç”¨ç”¨æˆ·è®¾ç½®çš„å‚æ•°å®ä¾‹åŒ–æ¨¡å‹
            model = model_dict[m](**model_params[m])
            best_params = None
            if use_search and m in param_grid and param_grid[m]:
                info_container.info(f"æ­£åœ¨è‡ªåŠ¨è°ƒå‚ï¼š{m} (GridSearchCV)")
                grid = {k: [float(x) if x.replace('.', '', 1).isdigit() else x for x in v if x] for k, v in param_grid[m].items()}
                search = GridSearchCV(model, grid, cv=cv_folds, scoring='roc_auc', n_jobs=-1)
                search.fit(X_train, y_train)
                model = search.best_estimator_
                best_params = search.best_params_
                st.write(f"æœ€ä¼˜å‚æ•°: {best_params}")
            else:
                model.fit(X_train, y_train)
            # äº¤å‰éªŒè¯
            if use_cv:
                cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)
                cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
                st.write(f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
            # æµ‹è¯•é›†è¯„ä¼°
            y_pred = model.predict(X_test)
            y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
            acc = accuracy_score(y_test, y_pred)
            # åˆ†ç±»æŠ¥å‘Šè¡¨æ ¼åŒ–
            report_dict = classification_report(y_test, y_pred, output_dict=True)
            # ä¸»è¦æŒ‡æ ‡æ”¶é›†
            metrics_dict[m] = {
                'å‡†ç¡®ç‡': acc,
                'F1åˆ†æ•°': report_dict['weighted avg']['f1-score'],
                'å¬å›ç‡': report_dict['weighted avg']['recall'],
                'ç²¾ç¡®ç‡': report_dict['weighted avg']['precision']
            }
            auc_value = None
            if y_proba is not None and len(np.unique(y_test)) == 2:
                fpr, tpr, _ = roc_curve(y_test, y_proba)
                auc_value = auc(fpr, tpr)
                auc_dict[m] = auc_value
            else:
                auc_dict[m] = None
            # æµ‹è¯•é›†æ‹Ÿåˆæ›²çº¿å›¾ç‰‡æ”¶é›†
            y_test_pred = y_pred
            fig_test, ax_test = plt.subplots()
            jitter = 0.1
            ax_test.scatter(y_test + np.random.uniform(-jitter, jitter, size=len(y_test)),
                            y_test_pred + np.random.uniform(-jitter, jitter, size=len(y_test_pred)),
                            alpha=0.5, color='orange', label='æµ‹è¯•é›†', marker='x')
            ax_test.set_xlabel('çœŸå®å€¼')
            ax_test.set_ylabel('é¢„æµ‹å€¼')
            ax_test.set_title(f'{m} æµ‹è¯•é›†æ‹Ÿåˆæ›²çº¿ï¼ˆçœŸå®å€¼-é¢„æµ‹å€¼ï¼‰')
            ax_test.set_xticks(np.unique(y_test))
            ax_test.set_yticks(np.unique(y_test_pred))
            ax_test.legend()
            test_pred_figs.append(fig_test)
            # å…¶ä½™å±•ç¤ºï¼ˆå¦‚2x3æ’ç‰ˆã€æ¨¡å‹ä¿å­˜ç­‰ï¼‰ä¿æŒä¸å˜
            # 2x3æ’ç‰ˆ
            row1, row2 = st.columns(3), st.columns(3)
            # æ··æ·†çŸ©é˜µ
            with row1[0]:
                fig_cm, ax_cm = plt.subplots()
                cm = confusion_matrix(y_test, y_pred)
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax_cm)
                ax_cm.set_xlabel('é¢„æµ‹ç±»åˆ«')
                ax_cm.set_ylabel('å®é™…ç±»åˆ«')
                ax_cm.set_title(f'{m} æ··æ·†çŸ©é˜µ')
                st.pyplot(fig_cm)
            # ç‰¹å¾é‡è¦æ€§
            with row1[1]:
                if hasattr(model, 'feature_importances_'):
                    importances = model.feature_importances_
                    feat_imp = pd.Series(importances, index=feature_cols).sort_values(ascending=False)
                    fig_imp, ax_imp = plt.subplots()
                    feat_imp.plot(kind='bar', ax=ax_imp)
                    ax_imp.set_title(f'{m} ç‰¹å¾é‡è¦æ€§')
                    ax_imp.set_ylabel('é‡è¦æ€§')
                    ax_imp.set_xlabel('ç‰¹å¾')
                    st.pyplot(fig_imp)
                elif hasattr(model, 'coef_'):
                    coef = model.coef_[0] if hasattr(model.coef_, '__len__') else model.coef_
                    feat_imp = pd.Series(coef, index=feature_cols).sort_values(ascending=False)
                    fig_imp, ax_imp = plt.subplots()
                    feat_imp.plot(kind='bar', ax=ax_imp)
                    ax_imp.set_title(f'{m} ç³»æ•°')
                    ax_imp.set_ylabel('ç³»æ•°')
                    ax_imp.set_xlabel('ç‰¹å¾')
                    st.pyplot(fig_imp)
                else:
                    st.info("è¯¥æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§å±•ç¤º")
            # ROCæ›²çº¿
            with row1[2]:
                if y_proba is not None and len(np.unique(y_test)) == 2:
                    fpr, tpr, _ = roc_curve(y_test, y_proba)
                    auc_value = auc(fpr, tpr)
                    fig_roc_single, ax_roc_single = plt.subplots()
                    ax_roc_single.plot(fpr, tpr, label=f'AUC={auc_value:.2f}')
                    ax_roc_single.plot([0, 1], [0, 1], 'k--')
                    ax_roc_single.set_xlabel('å‡æ­£ä¾‹ç‡(FPR)')
                    ax_roc_single.set_ylabel('çœŸæ­£ä¾‹ç‡(TPR)')
                    ax_roc_single.set_title(f'{m} ROCæ›²çº¿')
                    ax_roc_single.legend()
                    st.pyplot(fig_roc_single)
                else:
                    st.info("è¯¥æ¨¡å‹ä¸æ”¯æŒROCæ›²çº¿")
            # AUC
            with row2[0]:
                if auc_value is not None:
                    st.metric("AUC", f"{auc_value:.4f}")
                else:
                    st.info("æ— AUC")
            # è®­ç»ƒé›†æ‹Ÿåˆæ›²çº¿
            with row2[1]:
                y_train_pred = model.predict(X_train)
                fig_train, ax_train = plt.subplots()
                idxs = np.arange(len(y_train))
                ax_train.scatter(idxs, y_train, alpha=0.5, label='çœŸå®å€¼', marker='o', color='blue')
                ax_train.scatter(idxs, y_train_pred, alpha=0.5, label='é¢„æµ‹å€¼', marker='x', color='orange')
                ax_train.set_xlabel('æ ·æœ¬ç¼–å·')
                ax_train.set_ylabel('ç±»åˆ«')
                ax_train.set_title(f'{m} è®­ç»ƒé›†æ‹Ÿåˆæ›²çº¿ï¼ˆçœŸå®å€¼-é¢„æµ‹å€¼ï¼‰')
                ax_train.legend()
                st.pyplot(fig_train)
            # æµ‹è¯•é›†æ‹Ÿåˆæ›²çº¿
            with row2[2]:
                y_test_pred = y_pred
                idxs = np.arange(len(y_test))
                fig_test, ax_test = plt.subplots()
                ax_test.scatter(idxs, y_test, alpha=0.5, label='çœŸå®å€¼', marker='o', color='blue')
                ax_test.scatter(idxs, y_test_pred, alpha=0.5, label='é¢„æµ‹å€¼', marker='x', color='orange')
                ax_test.set_xlabel('æ ·æœ¬ç¼–å·')
                ax_test.set_ylabel('ç±»åˆ«')
                ax_test.set_title(f'{m} æµ‹è¯•é›†æ‹Ÿåˆæ›²çº¿ï¼ˆçœŸå®å€¼-é¢„æµ‹å€¼ï¼‰')
                ax_test.legend()
                st.pyplot(fig_test)

            # ä¿å­˜æ¨¡å‹
            results[m] = model
            progress_container.progress((idx + 1) / total, text=f"è®­ç»ƒè¿›åº¦ï¼š{idx + 1}/{total}")
        progress_container.empty()
        info_container.empty()
        # è®­ç»ƒåç»Ÿä¸€å±•ç¤ºä¸»è¦æŒ‡æ ‡è¡¨æ ¼
        st.subheader("å„æ¨¡å‹ä¸»è¦æŒ‡æ ‡å¯¹æ¯”")
        metrics_df = pd.DataFrame(metrics_dict).T
        metrics_df['AUC'] = pd.Series(auc_dict)
        metrics_df = metrics_df[['å‡†ç¡®ç‡', 'F1åˆ†æ•°', 'å¬å›ç‡', 'ç²¾ç¡®ç‡', 'AUC']]
        metrics_df = metrics_df.replace({None: np.nan})
        st.dataframe(metrics_df.style.format({col: "{:.4f}" for col in metrics_df.select_dtypes(include=[np.number]).columns}), use_container_width=True)
        # æµ‹è¯•é›†æ‹Ÿåˆæ›²çº¿å›¾ç‰‡åŒºåŸŸï¼Œ3åˆ—æ’å¸ƒï¼Œä¸ä¸Šé¢æ¯ä¸ªæ¨¡å‹çš„æµ‹è¯•é›†æ‹Ÿåˆæ›²çº¿ä¸€è‡´
        st.subheader("å„æ¨¡å‹æµ‹è¯•é›†æ‹Ÿåˆæ›²çº¿å¯¹æ¯”")
        for i, m in enumerate(selected_models):
            y_test_pred = results[m].predict(X_test)
            idxs = np.arange(len(y_test))
            fig, ax = plt.subplots()
            ax.scatter(idxs, y_test, alpha=0.5, label='çœŸå®å€¼', marker='o', color='blue')
            ax.scatter(idxs, y_test_pred, alpha=0.5, label='é¢„æµ‹å€¼', marker='x', color='orange')
            ax.set_xlabel('æ ·æœ¬ç¼–å·')
            ax.set_ylabel('ç±»åˆ«')
            ax.set_title(f'{m} æµ‹è¯•é›†æ‹Ÿåˆæ›²çº¿ï¼ˆçœŸå®å€¼-é¢„æµ‹å€¼ï¼‰')
            ax.legend()
            if i % 3 == 0:
                cols = st.columns(3)
            with cols[i % 3]:
                st.pyplot(fig)

        # ROCæ›²çº¿æ€»è§ˆ
        if len(selected_models) > 1 and ax_roc.has_data():
            ax_roc.plot([0, 1], [0, 1], 'k--')
            ax_roc.set_xlabel('å‡æ­£ä¾‹ç‡(FPR)')
            ax_roc.set_ylabel('çœŸæ­£ä¾‹ç‡(TPR)')
            ax_roc.set_title('ROCæ›²çº¿å¯¹æ¯”')
            ax_roc.legend()
            st.pyplot(fig_roc)
        # æ¨¡å‹å¯¼å‡º
        st.subheader("æ¨¡å‹å¯¼å‡º")
        for m, model in results.items():
            buf = pickle.dumps(model)
            st.download_button(f"ä¸‹è½½ {m} æ¨¡å‹", data=buf, file_name=f"{m}_model.pkl")
